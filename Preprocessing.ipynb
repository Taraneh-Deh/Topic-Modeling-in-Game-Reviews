{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48222fcb-abd9-41c2-8300-33fd9f9aa680",
   "metadata": {},
   "source": [
    "**Preprocessing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276eef8-f18d-42f5-b777-cc2bbb50e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import cmath\n",
    "import pandas as pd\n",
    "import re\n",
    "import symspellpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = []\n",
    "    if isinstance(text, str):\n",
    "        # 1. Eliminating handles, URLs, and numbers\n",
    "        text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "        text = re.sub(r\"\\d+\", \"\", text)  # Remove numeric characters\n",
    "        text = re.sub(r'@[^\\s]+|\\b(?:!+)\\b', '', text)\n",
    "        text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  # Split words based on camel case\n",
    "        text = re.sub(r'&gt|&lt', ' ', text)\n",
    "        text = re.sub(r'([a-z])\\1{2,}', r'\\1', text)\n",
    "        text = re.sub(r'(\\*|\\W\\*)+', '. ', text)  # Replacing asterisks (*) or asterisks with non-word characters with a dot (.) and a space\n",
    "        text = re.sub(r'\\(.*?\\)', '', text)  # Removing text within parentheses\n",
    "        text = re.sub(r'(\\W+)\\.', '. ', text)  # Replacing one or more non-word characters followed by a dot (.) with a single dot (.) and a space\n",
    "        text = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', text)  # Adding a space after a dot (.), question mark (?), or exclamation mark (!) if followed by a word character\n",
    "        text = re.sub(r'ing\\b', ' ', text)  # Replacing the word \"ing\" with a space\n",
    "        text = re.sub(r'\\b(product received for free\\.?|\\s+product received for free\\s+)\\b', '', text)  # Removing specific noise text\n",
    "        text = re.sub(r'(\\b\\w+\\b)( \\1)+', r'\\1', text)  # Removing repeated phrases if they occur consecutively\n",
    "\n",
    "        # 2. Tokenizing the string into words\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "        # 3. Removing stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.update(['game', 'play', 'player', 'time','a', 'for', 'i', 'the', 'expand', 'click', 'contain', 'spoiler', 'it', 'be', 'in', 'one', 'get', 'even', 'year', 'guess', 'see', 'got', 'feel', 'want', 'tell', 'absolute','every'])\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # 4. Removing punctuation\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "\n",
    "        # 5. Converting all words to lowercase\n",
    "        words = [word.lower() for word in words]\n",
    "\n",
    "        # 6. Lemmatization \n",
    "       #lemmatizer = WordNetLemmatizer()\n",
    "       #words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "        # 6. Typo correction\n",
    "        sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "        dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "        if not sym_spell.word_count:\n",
    "            sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "        words_fixed = []\n",
    "        for word in words:\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "            if suggestions:\n",
    "                words_fixed.append(suggestions[0].term)\n",
    "\n",
    "        return ' '.join(words_fixed)  # Join the preprocessed words back into a string\n",
    "\n",
    "\n",
    "# Reading dataset\n",
    "df = pd.read_excel('')\n",
    "\n",
    "# Preprocessing 'User Review' column\n",
    "tqdm.pandas()  # progress tracking\n",
    "df[''] = df['User Review'].progress_apply(lambda x: preprocess_text(x))\n",
    "\n",
    "# Displaying a sample of the preprocessed text\n",
    "print(\"Sample of preprocessed text:\")\n",
    "print(df['processed_review_with'].head())\n",
    "\n",
    "# Saving the preprocessed dataset to a new file\n",
    "df.to_excel('', index=False)\n",
    "print(\"Preprocessed dataset saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
